{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siddb/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import time\n",
    "from sklearn.linear_model import LogisticRegression #logistic regression\n",
    "from sklearn import svm #support vector Machine\n",
    "from sklearn.ensemble import RandomForestClassifier #Random Forest\n",
    "from sklearn.neighbors import KNeighborsClassifier #KNN\n",
    "from sklearn.naive_bayes import GaussianNB #Naive bayes\n",
    "from sklearn.tree import DecisionTreeClassifier #Decision Tree\n",
    "from sklearn.model_selection import train_test_split #training and testing data split\n",
    "from sklearn import metrics #accuracy measure\n",
    "from sklearn.metrics import confusion_matrix #for confusion matrix\n",
    "from sklearn import datasets, neighbors, linear_model, preprocessing\n",
    "from sklearn.model_selection import learning_curve, ShuffleSplit\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "from collections import Counter\n",
    "from imblearn import under_sampling, over_sampling\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import xgboost\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>ps_ind_01</th>\n",
       "      <th>ps_ind_02_cat</th>\n",
       "      <th>ps_ind_03</th>\n",
       "      <th>ps_ind_04_cat</th>\n",
       "      <th>ps_ind_05_cat</th>\n",
       "      <th>ps_ind_06_bin</th>\n",
       "      <th>ps_ind_07_bin</th>\n",
       "      <th>ps_ind_08_bin</th>\n",
       "      <th>...</th>\n",
       "      <th>ps_calc_11</th>\n",
       "      <th>ps_calc_12</th>\n",
       "      <th>ps_calc_13</th>\n",
       "      <th>ps_calc_14</th>\n",
       "      <th>ps_calc_15_bin</th>\n",
       "      <th>ps_calc_16_bin</th>\n",
       "      <th>ps_calc_17_bin</th>\n",
       "      <th>ps_calc_18_bin</th>\n",
       "      <th>ps_calc_19_bin</th>\n",
       "      <th>ps_calc_20_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target  ps_ind_01  ps_ind_02_cat  ps_ind_03  ps_ind_04_cat  \\\n",
       "0   7       0          2              2          5              1   \n",
       "1   9       0          1              1          7              0   \n",
       "2  13       0          5              4          9              1   \n",
       "3  16       0          0              1          2              0   \n",
       "4  17       0          0              2          0              1   \n",
       "\n",
       "   ps_ind_05_cat  ps_ind_06_bin  ps_ind_07_bin  ps_ind_08_bin       ...        \\\n",
       "0              0              0              1              0       ...         \n",
       "1              0              0              0              1       ...         \n",
       "2              0              0              0              1       ...         \n",
       "3              0              1              0              0       ...         \n",
       "4              0              1              0              0       ...         \n",
       "\n",
       "   ps_calc_11  ps_calc_12  ps_calc_13  ps_calc_14  ps_calc_15_bin  \\\n",
       "0           9           1           5           8               0   \n",
       "1           3           1           1           9               0   \n",
       "2           4           2           7           7               0   \n",
       "3           2           2           4           9               0   \n",
       "4           3           1           1           3               0   \n",
       "\n",
       "   ps_calc_16_bin  ps_calc_17_bin  ps_calc_18_bin  ps_calc_19_bin  \\\n",
       "0               1               1               0               0   \n",
       "1               1               1               0               1   \n",
       "2               1               1               0               1   \n",
       "3               0               0               0               0   \n",
       "4               0               0               1               1   \n",
       "\n",
       "   ps_calc_20_bin  \n",
       "0               1  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  \n",
       "\n",
       "[5 rows x 59 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"train.csv\")\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.036447517859182946"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"target\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data normalization\n",
    "- data split into train and cv set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for gini (evaluation per the problem description). \n",
    "- For the best accuracy, we want a normalized gini score to be as close as possible to 1. \n",
    "- The gini score for a random classifier is 0. \n",
    "- Also, if we get a negative gini score, we can just reverse the outcome to get a positive gini score of the same magnitude. \n",
    "- Gini score is a good metric for a skewed dataset such as this one. \n",
    "- For the evaluation, it needs the 'probability' from the classifier, not just the prediction (0 or 1)! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#helper functions\n",
    "def gini(actual, pred, cmpcol = 0, sortcol = 1):\n",
    "    assert( len(actual) == len(pred) )\n",
    "    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n",
    "    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n",
    "    totalLosses = all[:,0].sum()\n",
    "    giniSum = all[:,0].cumsum().sum() / totalLosses\n",
    "    giniSum -= (len(actual) + 1) / 2.\n",
    "    return giniSum / len(actual)\n",
    " \n",
    "def gini_normalized(a, p):\n",
    "    return gini(a, p) / gini(a, a)\n",
    "\n",
    "#results is a list of dictionaries\n",
    "def evaluate_clf(estimator, X_train, y_train, X_cv, y_cv, filename):\n",
    "    estimator_name = str(estimator).split(\"(\")[0]\n",
    "    fit = estimator.fit(X_train, y_train)\n",
    "    accuracy = fit.score(X_cv, y_cv)\n",
    "    gini_cv = gini_normalized(y_cv, estimator.predict_proba(X_cv)[:,1])\n",
    "    gini_train = gini_normalized(y_train, estimator.predict_proba(X_train)[:,1])\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(pd.Series(y_cv).values, estimator.predict(X_cv), pos_label=1, average='binary')\n",
    "    #print(precision)\n",
    "    filename = filename + '.pkl'\n",
    "    joblib.dump(estimator, filename)\n",
    "    results_list = [estimator_name, filename, accuracy, gini_cv, gini_train, precision, recall, fscore]\n",
    "    return results_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's normalize the data using scikitlearn preprocessing. Each column will have a mean of 0 and variance of 1. \n",
    "The output of normalizing is a numpy array, so we convert it back into a pandas dataframe. Note that we will not be normalizing 'y' values. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_np = scaler.fit_transform(data.drop([\"target\", \"id\"], axis=1))\n",
    "print(X_np.shape)\n",
    "normalized_data = pd.DataFrame(data=X_np, columns=data.drop([\"target\", \"id\"], axis=1).columns)\n",
    "normalized_data = pd.concat([normalized_data, data[\"target\"]], axis=1)\n",
    "normalized_data.describe()\n",
    "train,cv=train_test_split(normalized_data,test_size = 0.1, random_state=0,stratify=normalized_data['target'])\n",
    "train.describe()\n",
    "X_train = train.drop(\"target\", axis=1)\n",
    "y_train = train[\"target\"]\n",
    "X_cv = cv.drop(\"target\", axis = 1)\n",
    "y_cv = cv[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Vanilla logistic regression has a gini score of 0.2066 - much better than a random classifier.\n",
    "2. The first strategy to deal with imbalanced dataset is using class_weight = \"balanced\" in the classifier. This will automatically give more weight to the few positives in the data. This results in a gini score of 0.22 - much better than a random classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trial run\n",
    "estimator = LogisticRegression(C=1, class_weight = \"balanced\")\n",
    "#estimator = LogisticRegression(C=1)\n",
    "fit = estimator.fit(X_train, y_train)\n",
    "score = fit.score(X_cv, y_cv)\n",
    "print(score)\n",
    "print(classification_report(y_cv, estimator.predict(X_cv)))\n",
    "gini_cv = gini_normalized(y_cv, estimator.predict_proba(X_cv)[:,1])\n",
    "gini_train = gini_normalized(y_train, estimator.predict_proba(X_train)[:,1])\n",
    "print([gini_train, gini_cv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = ['estimator_name', 'filename', 'accuracy', 'gini_cv', 'gini_train', 'precision', 'recall', 'fscore']\n",
    "results_file = open('resultsfile.csv','a+')\n",
    "line = \",\".join(cols)\n",
    "results_file.write(line + '\\n')\n",
    "results_file.close()\n",
    "results = pd.DataFrame([], columns = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimators = [LogisticRegression(C = 1, class_weight = \"balanced\"), svm.LinearSVC(class_weight = \"balanced\", verbose = 2), DecisionTreeClassifier(max_depth = 3, class_weight = \"balanced\"),DecisionTreeClassifier(max_depth = 5, class_weight = \"balanced\"), RandomForestClassifier(max_depth=2, random_state=0, class_weight = \"balanced\", verbose = 2), KNeighborsClassifier(n_neighbors=3)]\n",
    "filenames = ['logistic_C1', 'SVC_C1_linear', 'DecTree_3','DecTree_5','RandForest', 'KNeigh_3']\n",
    "\n",
    "#for i in range(2,len(estimators)):\n",
    "for i in [0,2,3,4]:\n",
    "    estimator = estimators[i]\n",
    "    filename = filenames[i]\n",
    "    results_list =  evaluate_clf(estimator, X_train, y_train, X_cv, y_cv, filename)\n",
    "    print(results_list)\n",
    "    df = pd.DataFrame([results_list], columns = cols)\n",
    "    results = pd.concat([results, df],ignore_index=True)\n",
    "    line = str(results_list)[1:-1]\n",
    "    results_file = open('resultsfile.csv','a')\n",
    "    results_file.write(line + '\\n')\n",
    "    results_file.close()\n",
    "    \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i=1\n",
    "estimator = estimators[i]\n",
    "filename = filenames[i]\n",
    "results_list =  evaluate_clf(estimator, X_train, y_train, X_cv, y_cv, filename)\n",
    "print(results_list)\n",
    "df = pd.DataFrame([results_list], columns = cols)\n",
    "results = pd.concat([results, df],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = joblib.load(results['filename'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mbk = MiniBatchKMeans(init='k-means++', n_clusters=8, batch_size=1000,\n",
    "                      n_init=10, max_no_improvement=10, verbose=2)\n",
    "t0 = time.time()\n",
    "mbk.fit(X_train)\n",
    "t_mini_batch = time.time() - t0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\")\n",
    "cols_trimmed = [x for x in data.columns if 'calc' not in x]\n",
    "data_1 = data[cols_trimmed].drop(\"id\", axis = 1)\n",
    "scaler = StandardScaler()\n",
    "X_np = scaler.fit_transform(data_1.drop(\"target\", axis=1))\n",
    "print(X_np.shape)\n",
    "norm_X_data_1 = pd.DataFrame(data=X_np, columns=data_1.drop(\"target\", axis=1).columns)\n",
    "#normalized_data = pd.concat([normalized_data, data[\"target\"]], axis=1)\n",
    "norm_X_data_1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_plotpca(xdata, ydata, n_comp = 20):\n",
    "    pca = PCA(n_components=n_comp, svd_solver='full', random_state=1001)\n",
    "    X = pca.fit_transform(xdata)\n",
    "    print(\"variance explained in \" + str(n_comp) + \" components is \" + str(pca.explained_variance_ratio_.sum()))\n",
    "    print(pca.explained_variance_ratio_)\n",
    "    print(\"variance for first 2 compoenents = \" + str(pca.explained_variance_ratio_[:2].sum()))\n",
    "\n",
    "    #making the plot\n",
    "    y = ydata\n",
    "    colors = ['b', 'r']\n",
    "    target_names = np.unique(y)\n",
    "    for color, i, target_name in zip(colors, [0, 1], target_names):\n",
    "    #for color, i, target_name in zip([colors[0]], [0], [target_names[0]]):\n",
    "        plt.scatter(X[y == i, 0], X[y == i, 1], color=color, s=1, alpha=.8, label=target_name, marker='.')\n",
    "    plt.legend(loc='best', shadow=False, scatterpoints=3)\n",
    "    plt.title( \"Scatter plot of the training data projected on the 1st \"\n",
    "        \"and 2nd principal components\")\n",
    "    plt.xlabel(\"Principal axis 1 - Explains %.1f %% of the variance\" % (\n",
    "        pca.explained_variance_ratio_[0] * 100.0))\n",
    "    plt.ylabel(\"Principal axis 2 - Explains %.1f %% of the variance\" % (\n",
    "        pca.explained_variance_ratio_[1] * 100.0))\n",
    "\n",
    "    #plt.savefig('pca-porto-03.png', dpi=150)\n",
    "    plt.show()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fit_plotpca(normalized_data.drop(['target'], axis = 1),data['target'].values.astype(np.int8),n_comp = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_plotpca(norm_X_data_1, data_1['target'].values.astype(np.int8), n_comp = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data_1['target'].values.astype(np.int8)\n",
    "colors = ['b', 'r']\n",
    "target_names = np.unique(y)\n",
    "#for color, i, target_name in zip(colors, [0, 1], target_names):\n",
    "for color, i, target_name in zip([colors[0]], [0], [target_names[0]]):\n",
    "    plt.scatter(X[y == i, 0], X[y == i, 1], color=color, s=1,\n",
    "                alpha=.8, label=target_name, marker='.')\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=3)\n",
    "plt.title(\n",
    "        \"Scatter plot of the training data projected on the 1st \"\n",
    "        \"and 2nd principal components\")\n",
    "plt.xlabel(\"Principal axis 1 - Explains %.1f %% of the variance\" % (\n",
    "        pca.explained_variance_ratio_[0] * 100.0))\n",
    "plt.ylabel(\"Principal axis 2 - Explains %.1f %% of the variance\" % (\n",
    "        pca.explained_variance_ratio_[1] * 100.0))\n",
    "\n",
    "#plt.savefig('pca-porto-03.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried to visualize the data by doing a PCA. Selecting 20 features on the original dataset after normalization explains ~50% of the variability. For the reduced and normalized dataset - data_1, we can explain 79% of the variability. In the visualization in 2D, we will only be able to see 16.7 % of the variability. \n",
    "\n",
    "Plotting the data in 2D shows that the points labeled 1 clearly lie in the red cluster. Bad news is, this cluster lies entirely on top of the blue cluster - there is no separation, at least in these coordinates! Perhaps this is why we were not able to reach accuracies higher than ~50%. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried to visualize with t-SNE which can discover more complex features, but the code was aborted because too slow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=50.0, learning_rate=200.0, n_iter=1000, n_iter_without_progress=300, random_state=0)\n",
    "tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imbalanced Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the current dataset, linearSVM and kneighbors are both too slow - could not get them to run at all on my machine. Let us simplify the dataset.\n",
    "1. Features - remove all features that have calc in the name - as we had seen earlier using our heatmaps, these features have 0 correlation with the target.\n",
    "2. balancing the data. We will choose all of the row that have target = 1 and choose that number of rows with target = 0. So we will go from an imbalance of 1:32 to 1:1. Our data will also be a lot smaller, so should be much easier to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(ratio='majority', return_indices=False, random_state=None, replacement=False)\n",
    "X_resampled, y_resampled = rus.fit_sample(data_1.drop(['target'], axis = 1), data['target'])\n",
    "print(sorted(Counter(y_resampled).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_np = scaler.fit_transform(X_resampled)\n",
    "norm_X_resampled = pd.DataFrame(data=X_np, columns=data_1.drop([\"target\"], axis=1).columns)\n",
    "norm_Y_resampled = pd.DataFrame(data=y_resampled, columns = ['target'])\n",
    "norm_data_resampled = pd.concat([norm_X_resampled, norm_Y_resampled], axis=1)\n",
    "norm_data_resampled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train,cv=train_test_split(norm_data_resampled,test_size = 0.3, random_state=0,stratify=norm_data_resampled['target'])\n",
    "X_train = train.drop(\"target\", axis=1)\n",
    "y_train = train[\"target\"]\n",
    "X_cv = cv.drop(\"target\", axis = 1)\n",
    "y_cv = cv[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = ['estimator_name', 'filename', 'accuracy', 'gini_cv', 'gini_train', 'precision', 'recall', 'fscore']\n",
    "results_file = open('resultsfile2.csv','a+')\n",
    "line = \",\".join(cols)\n",
    "results_file.write(line + '\\n')\n",
    "results_file.close()\n",
    "results = pd.DataFrame([], columns = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "estimators = [CalibratedClassifierCV(svm.LinearSVC(class_weight = \"balanced\", verbose = 2)), DecisionTreeClassifier(max_depth = 7, class_weight = \"balanced\"),DecisionTreeClassifier(max_depth = 9, class_weight = \"balanced\"),KNeighborsClassifier(n_neighbors=5)]\n",
    "filenames = ['SVC_C1_linear_1', 'DecTree_7_1','DecTree_9_1','KNeigh_5_1']\n",
    "estimators = [GaussianNB()]\n",
    "filenames = ['GaussianNB_1']\n",
    "estimators = [LogisticRegression(C = 1, class_weight = \"balanced\"), CalibratedClassifierCV(svm.LinearSVC(class_weight = \"balanced\", verbose = 2)), DecisionTreeClassifier(max_depth = 3, class_weight = \"balanced\"),DecisionTreeClassifier(max_depth = 5, class_weight = \"balanced\"), RandomForestClassifier(max_depth=2, random_state=0, class_weight = \"balanced\", verbose = 2), KNeighborsClassifier(n_neighbors=3)]\n",
    "filenames = ['logistic_C1_1', 'SVC_C1_linear_1', 'DecTree_3_1','DecTree_5_1','RandForest_1', 'KNeigh_3_1']\n",
    "#for i in range(0,len(estimators)):\n",
    "#for i in [0,2,3,4,5]:\n",
    "for i in range(0,1):\n",
    "    estimator = estimators[i]\n",
    "    filename = filenames[i]\n",
    "    results_list =  evaluate_clf(estimator, X_train, y_train, X_cv, y_cv, filename)\n",
    "    print(results_list)\n",
    "    df = pd.DataFrame([results_list], columns = cols)\n",
    "    results = pd.concat([results, df],ignore_index=True)\n",
    "    line = str(results_list)[1:-1]\n",
    "    results_file = open('resultsfile2.csv','a')\n",
    "    results_file.write(line + '\\n')\n",
    "    results_file.close()\n",
    "    \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try visualizations again on this reduced dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = fit_plotpca(norm_data_resampled.drop(\"target\", axis = 1),norm_data_resampled['target'].values.astype(np.int8), 33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This still looks like a decent representation of the original dataset, but it is much smaller in size. We have gone from 600,000 to 40,000. Let's try to reduce this further to size of 5000 by undersampling both classes so that we can quickly try a variety of nonlinear models or non linear features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "choose = np.random.randint(0, norm_data_resampled.shape[0]-1, size=40000)\n",
    "norm_data_resampled_2 = norm_data_resampled.iloc[choose[:20000]]\n",
    "norm_data_resampled_2_cv = norm_data_resampled.iloc[choose[20000:]]\n",
    "n_comp = 33\n",
    "X = fit_plotpca(norm_data_resampled_2.drop(\"target\", axis = 1),norm_data_resampled_2['target'].values.astype(np.int8), n_comp)\n",
    "y = norm_data_resampled_2['target'].values.astype(np.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are at 5000 datapoints and the data still looks somewhat similar in PCA! Now we can try some fancy nonlinear stuff. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-linear features/algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt non-linear SVM with Kernel trick on the reduced dataset of size 5000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#results = pd.DataFrame([], columns = cols)\n",
    "cols = ['estimator_name', 'filename', 'accuracy', 'gini_cv', 'gini_train', 'precision', 'recall', 'fscore']\n",
    "estimators = [svm.SVC(C=1.0, kernel='rbf', gamma=0.01, probability = True), svm.SVC(C=1.0, kernel='rbf', gamma=0.1, probability = True), svm.SVC(C=1.0, kernel='rbf', gamma=100, probability = True)]\n",
    "filenames = ['SVM_C1_rbf0.01','SVM_C1_rbf0.1', 'SVM_C1_rbf100']\n",
    "#for i in range(0,len(estimators)):\n",
    "#for i in [0,2,3,4,5]:\n",
    "for i in range(0,1):\n",
    "    estimator = estimators[i]\n",
    "    filename = filenames[i]\n",
    "    t0 = time.time()\n",
    "    results_list =  evaluate_clf(estimator, norm_data_resampled_2.drop('target', axis = 1), norm_data_resampled_2['target'], norm_data_resampled_2_cv.drop('target', axis = 1), norm_data_resampled_2_cv['target'], filename)\n",
    "    t = time.time() - t0\n",
    "    print(\"time = \" + str(t) + \" s.\")\n",
    "    print(results_list)\n",
    "    df = pd.DataFrame([results_list], columns = cols)\n",
    "    results = pd.concat([results, df],ignore_index=True)\n",
    "    line = str(results_list)[1:-1]\n",
    "    results_file = open('resultsfile2.csv','a')\n",
    "    results_file.write(line + '\\n')\n",
    "    results_file.close()\n",
    "    \n",
    "    \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the first 33 principal components, of the dataset of size 5000, I will generate 33C2 additional features = 528 additional features. Then I will try PCA again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "iterator = list(itertools.combinations(range(0,n_comp), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_expand = X\n",
    "total = X.shape[1] + len(iterator)\n",
    "for cols in iterator:\n",
    "    #cols = iterator[1]\n",
    "    feature = np.reshape(X[:,cols[0]]*X[:,cols[1]], (-1,1))\n",
    "    X_expand = np.append(X_expand, feature, axis = 1)\n",
    "X_expand.shape\n",
    "#renormalize data\n",
    "scaler = StandardScaler()\n",
    "X_expand = scaler.fit_transform(X_expand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_expand = pd.DataFrame(data = X_expand, columns = range(0, total))\n",
    "X_expand.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = fit_plotpca(X_expand,norm_data_resampled_2['target'].values.astype(np.int8), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[y == 0, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    \n",
    "    y = norm_data_resampled_2['target'].values.astype(np.int8)\n",
    "    colors = ['b', 'r']\n",
    "    target_names = np.unique(y)\n",
    "    for color, i, target_name in zip(colors, [0, 1], target_names):\n",
    "    #for color, i, target_name in zip([colors[0]], [0], [target_names[0]]):\n",
    "        plt.scatter(X[y == i, 0], X[y == i, 1], color=color, s=1, alpha=0.8, label=target_name, marker='.')\n",
    "    plt.legend(loc='best', shadow=False, scatterpoints=3)\n",
    "    plt.title( \"Scatter plot of the training data projected on the 1st \"\n",
    "        \"and 2nd principal components\")\n",
    "    plt.xlabel(\"Principal axis 1 - Explains %.1f %% of the variance\" % (\n",
    "        pca.explained_variance_ratio_[0] * 100.0))\n",
    "    plt.ylabel(\"Principal axis 2 - Explains %.1f %% of the variance\" % (\n",
    "        pca.explained_variance_ratio_[1] * 100.0))\n",
    "    plt.axis([-5,5,-5,5])\n",
    "    #plt.savefig('pca-porto-03.png', dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't look like it helped at all in separating the two classes. We will stick to 33 features from the original PCA to try out other classification algos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "kpca = KernelPCA(kernel=\"poly\", n_jobs = -1)\n",
    "X_kpca = kpca.fit_transform(norm_data_resampled_2.drop(\"target\", axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_polypca = X_kpca\n",
    "reds = y == 0\n",
    "blues = y == 1\n",
    "plt.figure()\n",
    "plt.subplot(1, 1, 1, aspect='equal')\n",
    "plt.scatter(X_kpca[reds, 0], X_kpca[reds, 1], c=\"red\", s=40)\n",
    "plt.scatter(X_kpca[blues, 0], X_kpca[blues, 1], c=\"blue\", s=20)\n",
    "plt.title(\"Projection by KPCA\")\n",
    "plt.xlabel(\"1st principal component in space induced by $\\phi$\")\n",
    "plt.ylabel(\"2nd component\")\n",
    "#plt.subplots_adjust(0.02, 0.10, 0.98, 0.94, 0.04, 0.35)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first attempt at kPCA did not yield much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kpca = KernelPCA(kernel=\"sigmoid\", n_jobs = -1)\n",
    "X_kpca = kpca.fit_transform(norm_data_resampled_2.drop(\"target\", axis = 1))\n",
    "X_sigmoidpca = X_kpca\n",
    "reds = y == 0\n",
    "blues = y == 1\n",
    "plt.figure()\n",
    "plt.subplot(1, 1, 1, aspect='equal')\n",
    "plt.scatter(X_kpca[reds, 0], X_kpca[reds, 1], c=\"red\", s=40)\n",
    "plt.scatter(X_kpca[blues, 0], X_kpca[blues, 1], c=\"blue\", s=20)\n",
    "plt.title(\"Projection by KPCA\")\n",
    "plt.xlabel(\"1st principal component in space induced by $\\phi$\")\n",
    "plt.ylabel(\"2nd component\")\n",
    "#plt.subplots_adjust(0.02, 0.10, 0.98, 0.94, 0.04, 0.35)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid kernel gives 2 clouds, one pretty much on top of the other. We are looking for separation, so this doesn't help. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kpca = KernelPCA(kernel=\"rbf\", n_jobs = -1)\n",
    "X_kpca = kpca.fit_transform(norm_data_resampled_2.drop(\"target\", axis = 1))\n",
    "X_gausspca = X_kpca\n",
    "reds = y == 0\n",
    "blues = y == 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gaussian kernel seems like it helped somewhat, compared to what we saw for the sigmoid kernel. Perhaps some more tuning might help get better separation? First, let's replot this same one with smaller point sizes so we can more clearly see if there is really a separation. \n",
    "\n",
    "In general, I see more reds towards the left of the figure than to the right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.subplot(1, 1, 1, aspect='equal')\n",
    "plt.scatter(X_kpca[reds, 0], X_kpca[reds, 1], c=\"red\", s=10)\n",
    "plt.scatter(X_kpca[blues, 0], X_kpca[blues, 1], c=\"blue\", s=5)\n",
    "plt.title(\"Projection by KPCA\")\n",
    "plt.xlabel(\"1st principal component in space induced by $\\phi$\")\n",
    "plt.ylabel(\"2nd component\")\n",
    "#plt.subplots_adjust(0.02, 0.10, 0.98, 0.94, 0.04, 0.35)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.subplot(1, 1, 1, aspect='equal')\n",
    "plt.scatter(X_kpca[blues, 0], X_kpca[blues, 1], c=\"blue\", s=10)\n",
    "plt.scatter(X_kpca[reds, 0], X_kpca[reds, 1], c=\"red\", s=5)\n",
    "plt.title(\"Projection by KPCA\")\n",
    "plt.xlabel(\"1st principal component in space induced by $\\phi$\")\n",
    "plt.ylabel(\"2nd component\")\n",
    "#plt.subplots_adjust(0.02, 0.10, 0.98, 0.94, 0.04, 0.35)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gammas = [1, 10, 20] \n",
    "for g in gammas:\n",
    "    kpca = KernelPCA(kernel=\"rbf\", n_jobs = -1, gamma = g )\n",
    "    X_kpca = kpca.fit_transform(norm_data_resampled_2.drop(\"target\", axis = 1))\n",
    "    reds = y == 0\n",
    "    blues = y == 1\n",
    "    plt.figure()\n",
    "    plt.subplot(1, 1, 1, aspect='equal')\n",
    "    plt.scatter(X_kpca[blues, 0], X_kpca[blues, 1], c=\"blue\", s=10)\n",
    "    plt.scatter(X_kpca[reds, 0], X_kpca[reds, 1], c=\"red\", s=5)\n",
    "    plt.title(\"Projection by KPCA, gamma = \" + str(g))\n",
    "    plt.xlabel(\"1st principal component in space induced by $\\phi$\")\n",
    "    plt.ylabel(\"2nd component\")\n",
    "#plt.subplots_adjust(0.02, 0.10, 0.98, 0.94, 0.04, 0.35)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_pca_20 = X_kpca\n",
    "plt.figure()\n",
    "plt.subplot(1, 1, 1, aspect='equal')\n",
    "plt.scatter(X_kpca[blues, 0], X_kpca[blues, 1], c=\"blue\", s=10)\n",
    "plt.scatter(X_kpca[reds, 0], X_kpca[reds, 1], c=\"red\", s=5)\n",
    "plt.title(\"Projection by KPCA, gamma = \" + str(g))\n",
    "plt.xlabel(\"1st principal component in space induced by $\\phi$\")\n",
    "plt.ylabel(\"2nd component\")\n",
    "plt.axis([-.0025, 0.0025, -0.0025, 0.0025])\n",
    "#plt.subplots_adjust(0.02, 0.10, 0.98, 0.94, 0.04, 0.35)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(kpca.lambdas_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gammas = [.001, .1, 50]\n",
    "kpca = [0,0,0]\n",
    "for g,i in zip(gammas, range(0,3)):\n",
    "    kpca[i] = KernelPCA(kernel=\"rbf\", n_jobs = -1, gamma = g )\n",
    "    X_kpca = kpca[i].fit_transform(norm_data_resampled_2.drop(\"target\", axis = 1))\n",
    "    reds = y == 0\n",
    "    blues = y == 1\n",
    "    plt.figure()\n",
    "    plt.subplot(1, 1, 1, aspect='equal')\n",
    "    plt.scatter(X_kpca[blues, 0], X_kpca[blues, 1], c=\"blue\", s=10)\n",
    "    plt.scatter(X_kpca[reds, 0], X_kpca[reds, 1], c=\"red\", s=5)\n",
    "    plt.title(\"Projection by KPCA, gamma = \" + str(g))\n",
    "    plt.xlabel(\"1st principal component in space induced by $\\phi$\")\n",
    "    plt.ylabel(\"2nd component\")\n",
    "#plt.subplots_adjust(0.02, 0.10, 0.98, 0.94, 0.04, 0.35)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g,i in zip(gammas, range(0,3)):\n",
    "    #kpca[i] = KernelPCA(kernel=\"rbf\", n_jobs = -1, gamma = g )\n",
    "    X_kpca = kpca[i].transform(norm_data_resampled_2.drop(\"target\", axis = 1))\n",
    "    reds = y == 0\n",
    "    blues = y == 1\n",
    "    plt.figure()\n",
    "    plt.subplot(1, 1, 1, aspect='equal')\n",
    "    plt.scatter(X_kpca[blues, 0], X_kpca[blues, 1], c=\"blue\", s=10)\n",
    "    #plt.scatter(X_kpca[reds, 0], X_kpca[reds, 1], c=\"red\", s=5)\n",
    "    plt.title(\"Projection by KPCA, gamma = \" + str(g))\n",
    "    plt.xlabel(\"1st principal component in space induced by $\\phi$\")\n",
    "    plt.ylabel(\"2nd component\")\n",
    "#plt.subplots_adjust(0.02, 0.10, 0.98, 0.94, 0.04, 0.35)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = [20]\n",
    "kpca = [0,0,0]\n",
    "for g,i in zip(gammas, range(0, len(gammas))):\n",
    "    kpca[i] = KernelPCA(kernel=\"rbf\", n_jobs = -1, gamma = g, n_components = 100 )\n",
    "    X_kpca = kpca[i].fit_transform(norm_data_resampled_2.drop(\"target\", axis = 1))\n",
    "    X_back = kpca.inverse_transform(X_kpca)\n",
    "    reds = y == 0\n",
    "    blues = y == 1\n",
    "    plt.figure()\n",
    "    plt.subplot(2, 2, 1, aspect='equal')\n",
    "    plt.scatter(X_kpca[blues, 0], X_kpca[blues, 1], c=\"blue\", s=10)\n",
    "    plt.scatter(X_kpca[reds, 0], X_kpca[reds, 1], c=\"red\", s=5)\n",
    "    plt.title(\"Projection by KPCA, gamma = \" + str(g))\n",
    "    plt.xlabel(\"1st principal component in space induced by $\\phi$\")\n",
    "    plt.ylabel(\"2nd component\")\n",
    "#plt.subplots_adjust(0.02, 0.10, 0.98, 0.94, 0.04, 0.35)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_cols(X):\n",
    "    cols_trimmed = [i for i in range(0, len(data.drop('target', axis = 1).columns)) if 'calc' not in data.columns[i] and 'id' not in data.columns[i]]\n",
    "    X_trimmed = X[:, cols_trimmed]\n",
    "    return X_trimmed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipe = make_pipeline(FunctionTransformer(drop_cols), StandardScaler(), svm.SVC(kernel='rbf', probability = True)) \n",
    "pipe = make_pipeline(FunctionTransformer(drop_cols), StandardScaler(), LogisticRegression(class_weight = \"balanced\")) \n",
    "param_grid = dict(logisticregression__C = [0.01, 1, 100])\n",
    "grid_search = GridSearchCV(pipe, param_grid=param_grid, scoring='roc_auc', refit=True, cv=5, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_search.fit(data.drop('target', axis = 1),data['target'])\n",
    "grid_search.best_estimator_.steps[2]\n",
    "resultsdf = pd.DataFrame(grid_search.cv_results_)\n",
    "resultsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted = grid_search.predict_proba(data.drop('target', axis = 1))\n",
    "gini = gini_normalized(data['target'], predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('functiontransformer', FunctionTransformer(accept_sparse=False,\n",
       "            func=<function drop_cols at 0x1a173366a8>, inv_kw_args=None,\n",
       "            inverse_func=None, kw_args=None, pass_y='deprecated',\n",
       "            validate=True)),\n",
       " ('xgbclassifier',\n",
       "  XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "         gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "         min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
       "         objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "         scale_pos_weight=1, seed=0, silent=True, subsample=1))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = xgboost.XGBClassifier()\n",
    "pipe2 = make_pipeline(FunctionTransformer(drop_cols), clf) \n",
    "pipe2.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "[CV] xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__scale_pos_weight=25 \n",
      "[CV] xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__scale_pos_weight=25 \n",
      "[CV]  xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__scale_pos_weight=25, total=   6.8s\n",
      "[CV]  xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__scale_pos_weight=25, total=   6.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:    8.9s finished\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-94de2a51dd09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"roc_auc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mresultsdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mresultsdf2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "#param_grid = dict(xgbclassifier__n_estimators = range(50, 400, 50), xgbclassifier__max_depth = [3,5,7], scale_pos_weight=[25, 35, 40])\n",
    "#kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\n",
    "param_grid = dict(xgbclassifier__n_estimators = range(50, 100, 50), xgbclassifier__max_depth = [3], xgbclassifier__scale_pos_weight=[25])\n",
    "kfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=7)\n",
    "grid_search = GridSearchCV(pipe2, param_grid, scoring=\"roc_auc\", n_jobs=-1, cv=kfold, refit=True, verbose=2)\n",
    "grid_search.fit(data.drop('target', axis = 1),data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siddb/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/siddb/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/siddb/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/siddb/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_xgbclassifier__max_depth</th>\n",
       "      <th>param_xgbclassifier__n_estimators</th>\n",
       "      <th>param_xgbclassifier__scale_pos_weight</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.98209</td>\n",
       "      <td>0.874085</td>\n",
       "      <td>0.633282</td>\n",
       "      <td>0.645942</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>25</td>\n",
       "      <td>{'xgbclassifier__max_depth': 3, 'xgbclassifier...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.633133</td>\n",
       "      <td>0.646222</td>\n",
       "      <td>0.633432</td>\n",
       "      <td>0.645661</td>\n",
       "      <td>0.175571</td>\n",
       "      <td>0.130244</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0        5.98209         0.874085         0.633282          0.645942   \n",
       "\n",
       "  param_xgbclassifier__max_depth param_xgbclassifier__n_estimators  \\\n",
       "0                              3                                50   \n",
       "\n",
       "  param_xgbclassifier__scale_pos_weight  \\\n",
       "0                                    25   \n",
       "\n",
       "                                              params  rank_test_score  \\\n",
       "0  {'xgbclassifier__max_depth': 3, 'xgbclassifier...                1   \n",
       "\n",
       "   split0_test_score  split0_train_score  split1_test_score  \\\n",
       "0           0.633133            0.646222           0.633432   \n",
       "\n",
       "   split1_train_score  std_fit_time  std_score_time  std_test_score  \\\n",
       "0            0.645661      0.175571        0.130244        0.000149   \n",
       "\n",
       "   std_train_score  \n",
       "0         0.000281  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_.steps[1]\n",
    "resultsdf2 = pd.DataFrame(grid_search.cv_results_)\n",
    "resultsdf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted = grid_search.predict_proba(data.drop('target', axis = 1))\n",
    "gini = gini_normalized(data['target'], predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.28171316094087606"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 63 candidates, totalling 315 fits\n",
      "[CV] xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__scale_pos_weight=25 \n",
      "[CV] xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__scale_pos_weight=25 \n",
      "[CV] xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__scale_pos_weight=25 \n",
      "[CV] xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__scale_pos_weight=25 \n",
      "[CV] xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__scale_pos_weight=25 \n",
      "[CV] xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__scale_pos_weight=35 \n",
      "[CV] xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__scale_pos_weight=35 \n",
      "[CV] xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__scale_pos_weight=35 \n"
     ]
    }
   ],
   "source": [
    "param_grid = dict(xgbclassifier__n_estimators = range(50, 400, 50), xgbclassifier__max_depth = [3,5,7], xgbclassifier__scale_pos_weight=[25, 35, 40])\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\n",
    "#param_grid = dict(xgbclassifier__n_estimators = range(50, 100, 50), xgbclassifier__max_depth = [3], scale_pos_weight=[25])\n",
    "#kfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=7)\n",
    "grid_search = GridSearchCV(pipe2, param_grid, scoring=\"roc_auc\", n_jobs=-1, cv=kfold, refit=True, verbose=2)\n",
    "grid_search.fit(data.drop('target', axis = 1),data['target'])\n",
    "grid_search.best_estimator_.steps[1]\n",
    "resultsdf3 = pd.DataFrame(grid_search.cv_results_)\n",
    "resultsdf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted = grid_search.predict_proba(data.drop('target', axis = 1))\n",
    "gini = gini_normalized(data['target'], predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Making predictions on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "cols_trimmed = [x for x in test.columns if 'calc' not in x and 'id' not in x]\n",
    "test_1 = test[cols_trimmed]\n",
    "scaler = StandardScaler()\n",
    "X_np = scaler.fit_transform(test_1)\n",
    "norm_test = pd.DataFrame(data=X_np, columns=cols_trimmed)\n",
    "clf = joblib.load('SVM_C1_rbf0.01.pkl')\n",
    "t0= time.time()\n",
    "y = clf.predict_proba(norm_test)\n",
    "t = time.time()-t0\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "y = grid_search.predict_proba(test)\n",
    "df = pd.DataFrame.from_items([(\"target\", y[:,1])])\n",
    "test_results = pd.concat([test[\"id\"], df], axis = 1)\n",
    "test_results.to_csv(\"submission_logistic.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
